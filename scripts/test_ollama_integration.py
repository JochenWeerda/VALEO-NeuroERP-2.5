"""
Test-Skript f√ºr die Ollama-Integration im VAN-Modus.
Testet die Verbindung zu lokalen LLMs √ºber Ollama.
"""

import os
import sys
import asyncio
import logging
from pathlib import Path

# Lokales Repo-Verzeichnis priorisieren
repo_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(repo_root))

from backend.apm_framework.llm_service import LLMService, LLMProvider

# Logging konfigurieren
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_ollama_connection():
    """Testet die Verbindung zu Ollama."""
    try:
        # Ollama-Service initialisieren
        llm_service = LLMService(LLMProvider.OLLAMA)
        logger.info("Ollama-Service initialisiert")
        
        # Verf√ºgbare Modelle pr√ºfen
        base_url = llm_service.base_url
        logger.info(f"Ollama Base-URL: {base_url}")
        
        # Test-Anforderung
        test_requirement = "Als Disponent m√∂chte ich eingehende Bestellungen automatisch pr√ºfen lassen."
        
        logger.info("Teste Anforderungsanalyse mit Ollama...")
        analysis = await llm_service.analyze_requirement(
            test_requirement,
            context={"project_id": "test-ollama", "test": True}
        )
        
        logger.info("‚úÖ Anforderungsanalyse erfolgreich!")
        print("\n=== Ollama-Analyse ===")
        print(analysis)
        
        # Teste Kl√§rungsfragen
        logger.info("Teste Generierung von Kl√§rungsfragen...")
        questions = await llm_service.generate_clarification_questions(
            analysis,
            context={"project_id": "test-ollama", "test": True}
        )
        
        logger.info("‚úÖ Kl√§rungsfragen erfolgreich generiert!")
        print(f"\n=== Kl√§rungsfragen ({len(questions)}) ===")
        for i, question in enumerate(questions, 1):
            print(f"{i}. {question}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Fehler bei Ollama-Test: {str(e)}")
        return False


async def test_ollama_models():
    """Listet verf√ºgbare Ollama-Modelle auf."""
    try:
        import aiohttp
        
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        
        async with aiohttp.ClientSession() as session:
            # Verf√ºgbare Modelle abrufen
            async with session.get(f"{base_url}/api/tags") as response:
                if response.status == 200:
                    models_data = await response.json()
                    models = models_data.get("models", [])
                    
                    print(f"\n=== Verf√ºgbare Ollama-Modelle ({len(models)}) ===")
                    for model in models:
                        name = model.get("name", "Unbekannt")
                        size = model.get("size", 0)
                        size_mb = size / (1024 * 1024) if size > 0 else 0
                        print(f"üì¶ {name} ({size_mb:.1f} MB)")
                    
                    return models
                else:
                    logger.error(f"Fehler beim Abrufen der Modelle: {response.status}")
                    return []
                    
    except Exception as e:
        logger.error(f"Fehler beim Abrufen der Modelle: {str(e)}")
        return []


async def test_ollama_van_mode():
    """Testet den VAN-Modus mit Ollama."""
    try:
        from backend.apm_framework.mongodb_connector import APMMongoDBConnector
        from backend.apm_framework.van_mode import VANMode
        
        # MongoDB-Connector (Mock f√ºr Test)
        mongo_uri = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
        mongo_db = os.getenv("MONGODB_DB", "valeo_neuroerp")
        
        connector = APMMongoDBConnector(mongo_uri, mongo_db)
        
        # VAN-Modus mit Ollama initialisieren
        van_mode = VANMode(connector, "test-ollama", "ollama")
        
        logger.info("VAN-Modus mit Ollama initialisiert")
        
        # Test-Anforderung
        test_requirement = """
        Als Disponent im VALEO NeuroERP m√∂chte ich, dass eingehende Kundenbestellungen 
        automatisch auf Vollst√§ndigkeit und Plausibilit√§t gepr√ºft werden. 
        Bei Unklarheiten sollen automatisch pr√§zise Kl√§rungsfragen generiert werden.
        """
        
        logger.info("F√ºhre VAN-Analyse mit Ollama aus...")
        result = await van_mode.run(test_requirement)
        
        logger.info("‚úÖ VAN-Analyse mit Ollama erfolgreich!")
        print("\n=== VAN-Ergebnis mit Ollama ===")
        print(f"ID: {result.get('id', 'N/A')}")
        print(f"LLM-Provider: {result.get('llm_provider', 'N/A')}")
        print(f"Kl√§rungsfragen: {len(result.get('clarifications', []))}")
        print(f"√Ñhnliche Anforderungen: {len(result.get('similar_requirements', []))}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Fehler bei VAN-Modus-Test: {str(e)}")
        return False


async def main():
    """Hauptfunktion f√ºr alle Tests."""
    print("üöÄ Ollama-Integration Test f√ºr VALEO NeuroERP VAN-Modus")
    print("=" * 60)
    
    # Umgebungsvariablen setzen
    os.environ.setdefault("OLLAMA_BASE_URL", "http://localhost:11434")
    os.environ.setdefault("OLLAMA_MODEL", "llama3.2:3b")
    
    print(f"üîß Konfiguration:")
    print(f"   Ollama URL: {os.getenv('OLLAMA_BASE_URL')}")
    print(f"   Standard-Modell: {os.getenv('OLLAMA_MODEL')}")
    print()
    
    # Test 1: Ollama-Verbindung
    print("üì° Test 1: Ollama-Verbindung")
    connection_ok = await test_ollama_connection()
    print()
    
    # Test 2: Verf√ºgbare Modelle
    print("üì¶ Test 2: Verf√ºgbare Modelle")
    models = await test_ollama_models()
    print()
    
    # Test 3: VAN-Modus Integration
    print("üß† Test 3: VAN-Modus mit Ollama")
    van_ok = await test_ollama_van_mode()
    print()
    
    # Zusammenfassung
    print("üìä Test-Zusammenfassung")
    print("=" * 30)
    print(f"‚úÖ Ollama-Verbindung: {'OK' if connection_ok else 'FEHLER'}")
    print(f"‚úÖ Verf√ºgbare Modelle: {len(models)} gefunden")
    print(f"‚úÖ VAN-Modus: {'OK' if van_ok else 'FEHLER'}")
    
    if connection_ok and van_ok:
        print("\nüéâ Alle Tests erfolgreich! Ollama ist bereit f√ºr den VAN-Modus.")
        print("\nüí° N√§chste Schritte:")
        print("   1. Demo mit Ollama starten: LLM_PROVIDER=ollama python scripts/run_van_workflow_demo.py")
        print("   2. Verschiedene Modelle testen: OLLAMA_MODEL=mistral:7b python scripts/run_van_workflow_demo.py")
        print("   3. Frontend mit Ollama-Integration starten")
    else:
        print("\n‚ö†Ô∏è  Einige Tests fehlgeschlagen. Bitte pr√ºfen Sie:")
        print("   1. Ist Ollama installiert und l√§uft?")
        print("   2. Ist der Standardport 11434 verf√ºgbar?")
        print("   3. Sind Modelle heruntergeladen? (ollama pull llama3.2:3b)")


if __name__ == "__main__":
    asyncio.run(main())
